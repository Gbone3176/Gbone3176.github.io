---
title: 'DDPM学习笔记'
date: 2025-02-12
permalink: /posts/2025/02/DDPM学习笔记/
tags:
  - diffusion model
---
# DDPM学习笔记
1.DDPM
DDPM中的一些基础概念
1. 先验和后验概率：
事件A（交通事故，原因）在一定程度上会导致事件B（堵车，结果）的发生，先验概率(由因所果)：我们已经知道发生了事件A（交通事故），去推测可能会导致B（堵车）的概率，即P(B|A)；后验概率(执果所因)：已经发生了事件B（堵车了），推测是由于事件A（交通事故）导致的概率，即P(A|B)。
1. Reparameterization 重参数技巧
避免在采样过程中梯度的消失，
- 连续的情况，对于高斯分布来说，似乎就是把对带参数分布的直接采样转换成对不带参数分布的采样加上参数的特征（乘以方差加均值）。
- 离散的情况: 漫谈重参数:从正态分布到Gumbel Softmax - 科学空间|Scientific Spaces

DDPM李宏毅视频笔记
part1.生成噪声和去噪过程
[图片]
每一个Denoise Block输入一个带有噪声的图片和一个时间标记
[图片]
推理的时候的输入
[图片]
 整体的噪声估计和去噪
part2. 影像生成过程
1.最大似然估计-找到一个符合预期的”生成图像分布“
[图片]
[图片]
这里将分布生成的概率直接转化成了两个分布之间的距离(相似程度)- KL divergence 【第三行最后一项可以理解为取了N多次 x ，最终得到实验的概率$$logP_\theta(x)$$的均值（感觉上面应该写错了）就会向整体分布的期望$$E_{x~P_{datd}}[logP_\theta(x)]$$靠拢，也就转化成了取得x就是原始分布$$P_{data}$$的期望】（类似大数定理）
最后一步从max变成min是因为变换的时候少了一个负号。
那么如何去计算$$P_\theta(x)$$的大小？如果直接用G(z)去对应x，那么完全对应上了才为1，否则都为0，两级分化的结果实际上不好作为模型的评判标准，因此在这里将G(z)作为一个高斯分布的均值，用这个分部中心（也就是均值）与某一个x的距离的大小作为批判模型好坏的标准
[图片]
让$$P(x)$$最小的x通常无法直接求得（也可能是为了简化计算），而是通常使用$$Lower Bound$$进行缩放。以VAE来举例如下：这里的$$q(*)$$可以是任意的，对应的就是VAE的Encoder部分。
[图片]
DDPM也是一样的，可以得到其$$Lower~~Bound$$:
[图片]
train的过程中，DDPM的每一个Denoise block相当于上面VAE的network ，输入是上一个带噪声的$$x_t$$,输出是一个高斯的均值（更确切地说是$$(x_{t-1})_\theta=x_t-\epsilon_\theta,(x_{t-1})_\theta就是G(x_t)$$），用这个均值和真实的$$t-1$$步带噪声的图片的分布的均值的距离来衡量模型的好坏。
[图片]
怎么算这个$$P_\theta(x_{t-1}|x_t)$$?

diffusion过程$$q(x_t|x_{t-1})$$
设定一组超参$$[\beta_1,\beta_2,\beta_3,...,\beta_T]$$,对于其中某一步$$q(x_t|x_{t-1})$$，由以下式子构成：
$$q(x_t|x_{t-1})$$的mean为$$\sqrt{1-\beta_t}$$,var为$$\sqrt{\beta_t}$$
[图片]
如何计算$$q(x_t|x_0)$$？不需要一步一步的迭代：
[图片]
两次加噪是从同一个分布中采样，可以合并成一次采样（待证明）
[图片]
从0到t就可以得到以下过程（记$$\alpha_t=1-\beta_t$$，$$\overline\alpha_t=\alpha_1\alpha_2\alpha_3...\alpha_t$$）
[图片]
[图片]
现在再来看Lower Bound:
[图片]

红色框利用了全概率公式和马尔可夫性质

[图片]
红色框的两项是可以通过学习一个$$\theta
$$来降到最小的，而绿色的只和diffusion有关而和denoising无关，不用管。
两个红框计算过程类似，看第二个怎么算
[图片]
下面的红色框同样利用了全概率公式和马尔可夫性质，再利用高斯分布的式子直接硬算这最后一个式子，可以得到其仍然为一个高斯分布，均值和方差如下（这里的$$x_{t-1}$$是第 t-1 步噪声预测时的真实值）
[图片]
接下来计算这两个高斯分布的KL距离，可以直接套公式：
[图片]
但是要达到使得这一项最小的这样一个目的，并不是一定要计算出这一项的大小，可以通过另一种方法：既然同为高斯分布，可以想办法从均值和方差上直接让两个分布接近：
这个q()的分布的mean和var都是人工设定（已知）的，而P()的variance不需要管（说是管了也没用，影响不大，但是在IDDPM中进行了优化）那么只需要让两个的mean尽可能靠近即可。
[图片]
相当于就是通过x0和xt求出了xt-1的真实值，然后让denoise网络只接受一个xt和time，产生的输出越接近xt-1的真实值越好（DDPM是一步一步来，如果直接输出x0就有点像VAE了）
[图片]
利用$$x_t$$和$$x_0$$之间的关系还可以进一步化简上面这个式子：
[图片]
最后得到：
[/images/image-alignment-580x300.jpg]
那么，Denoise网络所需要做的就是预测一个$$\epsilon$$噪声出来就可以了，根据Lee老师推测，这个加上一个$$\sigma_t z$$的目的是提高生成结果的多样性，避免输出的单调